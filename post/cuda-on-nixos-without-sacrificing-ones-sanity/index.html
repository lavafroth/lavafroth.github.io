<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Painlessly setting up ML tooling on NixOS - lavafroth</title><meta name=description content='
Note: Use the following method only if you wish to have the latest version of CUDA that is
not yet available in the nix-community cache, otherwise follow this.


TL;DR: Save this flake, run nix develop and setup PyTorch as described

CUDA is a proprietary vendor lock-in for machine learning folks.
Training ML models is incredibly fast with CUDA as compared to CPUs due to the parallel
processing. So if you&rsquo;re doing something serious, you have no other choice besides CUDA as of writing.
Although, OpenAI&rsquo;s Triton and ZLUDA are worth keeping an eye on.'><meta name=author content><link rel="preload stylesheet" as=style href=https://lavafroth.is-a.dev/app.min.css><link rel=preload as=image href=../../header.svg><link as=font href=https://lavafroth.is-a.dev/latinmodern-math.otf><link rel=preload as=image href=https://lavafroth.is-a.dev/github.svg><link rel=preload as=image href=https://lavafroth.is-a.dev/about.svg><link rel=preload as=image href=https://lavafroth.is-a.dev/art.svg><link rel=preload as=image href=https://lavafroth.is-a.dev/rss.svg><link rel=icon href=https://lavafroth.is-a.dev/favicon.png><link rel=blog-icon href=https://lavafroth.is-a.dev/icon.png></head><body><header><a class=site-name href=https://lavafroth.is-a.dev/><svg viewBox="0 0 8790 2400"><path d="M80 1935V465h216v1270h286v2e2zm853 0 222-1470h264l222 1470h-210l-40-3e2h-208l-40 3e2zm280-528h148l-62-494-6-78h-12l-6 78zm1025 528L2014 465h210l108 868 8 142h12l8-142 108-868h210l-224 1470zm813 0 222-1470h264l222 1470h-210l-40-3e2h-208l-40 3e2zm280-528h148l-62-494-6-78h-12l-6 78zm851 528V465h514v222h-298v386h2e2v222h-2e2v640zm910 0V465h216q194 0 286 108 92 107 92 316 0 124-43 215-44 90-106 132l147 699h-216l-122-620h-38v620zm216-820q60 0 95-26 35-27 50-76t15-116q0-105-34-161-35-57-126-57zm1084 836q-90 0-154-42-65-42-99-114-35-72-35-162V767q0-91 35-162 34-72 99-114 64-42 154-42t155 42q64 42 99 114 34 72 34 162v866q0 90-34 162-35 72-99 114-65 42-155 42zm0-210q40 0 56-33 16-34 16-75V767q0-41-17-74-17-34-55-34-37 0-54 34-18 33-18 74v866q0 41 17 75 17 33 55 33zm890 194V687h-204V465h624v222h-204v1248zm828 0V465h216v608h168V465h216v1470h-216v-640h-168v640z"/></svg></a><nav><a style=--url:url(./github.svg) href=https://github.com/lavafroth aria-label=github target=_blank></a><a href=../../about/ aria-label=about style=--url:url(./about.svg)></a><a href=../../art/ aria-label=art style=--url:url(./art.svg)></a><a href=../../index.xml aria-label=rss style=--url:url(./rss.svg)></a><nav></header><main><article class=post-single data-pagefind-body><hgroup><p data-pagefind-ignore><time>Aug 10, 2024 | 4 minutes read</time></p><h1 data-pagefind-meta=title>Painlessly setting up ML tooling on NixOS</h1></hgroup><section class=post-content><blockquote><p>Note: Use the following method only if you wish to have the latest version of CUDA that is
not yet available in the nix-community cache, otherwise follow <a href=https://nix-community.org/cache>this</a>.</p></blockquote><blockquote><p><em>TL;DR:</em> Save <a href=#the-flake>this flake</a>, run <code>nix develop</code> and <a href=#setting-up-pytorch>setup PyTorch as described</a></p></blockquote><p><a href=https://en.wikipedia.org/wiki/CUDA>CUDA</a> is a proprietary vendor lock-in for machine learning folks.
Training ML models is incredibly fast with CUDA as compared to CPUs due to the parallel
processing. So if you&rsquo;re doing something serious, you have no other choice besides CUDA as of writing.
Although, OpenAI&rsquo;s Triton and ZLUDA are worth keeping an eye on.</p><p>Unlike your average distro, Nix will prevents conflicts between installed packages by storing its <abbr title="packages and libraries">derivations</abbr> in the <a href=https://zero-to-nix.com/concepts/nix-store>Nix store</a> instead of
locations like <code>/usr/bin</code>, <code>/usr/lib</code> and <code>/usr/lib64</code>.</p><h1 id=how-not-to-add-cuda>How not to add CUDA</h1><p>CUDA, being proprietary junk, does not allow you to redistribute
binaries that are linked with its blobs. Thus, for CUDA enabled PyTorch, we would have to <a href=https://discourse.nixos.org/t/pytorch-and-cuda-torch-not-compiled-with-cuda-enabled/11272/2>allow unfree
packages and enable CUDA support</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-nix data-lang=nix><span style=display:flex><span><span style=color:#f92672>import</span> sources<span style=color:#f92672>.</span>nixpkgs {
</span></span><span style=display:flex><span>  config <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    allowUnfree <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span>;
</span></span><span style=display:flex><span>    cudaSupport <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span>;
</span></span><span style=display:flex><span>  };
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Adding this to our <code>flake.nix</code> allows us the include these packages:</p><ul><li><code>linuxPackages.nvidia_x11</code></li><li><code>cudatoolkit</code></li><li><code>cudnn</code></li></ul><p>Now we can install PyTorch by either adding</p><ul><li><code>python311Packages.pytorch</code> to build PyTorch from source with CUDA support. This will take time longer than the heat death of the universe and more likely freeze low end PCs.
Refer to <a href="https://news.ycombinator.com/item?id=32931486">this hackernews post</a>.</li><li><code>python311Packages.pytorch-bin</code> which some people claim to have slightly faster builds at it
fetches the PyTorch binary from pytorch.org and patches it with the CUDA from <code>/nix/store</code>.
Refer to <a href=https://www.reddit.com/r/NixOS/comments/195pzdb/speeding_up_python311packagestorchwithcuda_build/>this reddit post</a>.</li></ul><p>Both of these approaches are extremely slow, you might have to leave your PC overnight to actually get it to work.</p><h1 id=bending-the-rules>Bending the rules</h1><p>To avoid all of the pain, we can build a lightweight sandbox that follows the normal Filesystem Hierarchy Standard with directories like <code>/usr/bin</code>, <code>/usr/lib</code>, etc.
Nix allows you to create such isolated root filesystems using the <a href=https://ryantm.github.io/nixpkgs/builders/special/fhs-environments/><code>pkgs.buildFHSEnv</code></a> function.</p><p>It accepts a <code>name</code> for the environment and a list of <code>targetPkgs</code> with the things we&rsquo;d need for basic NVIDIA support.
Note the inclusion of <code>micromamba</code> which will do most of the legwork when setting up PyTorch.
I&rsquo;ve also included the <code>fish</code> shell because that&rsquo;s what I daily drive. You can remove that and the <code>runScript</code> attribute
to use the default bash.</p><h2 id=the-flake>The flake</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-nix data-lang=nix><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      description <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Python 3.11 development environment&#34;</span>;
</span></span><span style=display:flex><span>      outputs <span style=color:#f92672>=</span> { self<span style=color:#f92672>,</span> nixpkgs }:
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span>
</span></span><span style=display:flex><span>        system <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;x86_64-linux&#34;</span>;
</span></span><span style=display:flex><span>        pkgs <span style=color:#f92672>=</span> <span style=color:#f92672>import</span> nixpkgs {
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>inherit</span> system;
</span></span><span style=display:flex><span>          config<span style=color:#f92672>.</span>allowUnfree <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span>;
</span></span><span style=display:flex><span>        };
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>in</span> {
</span></span><span style=display:flex><span>        devShells<span style=color:#f92672>.</span><span style=color:#e6db74>${</span>system<span style=color:#e6db74>}</span><span style=color:#f92672>.</span>default <span style=color:#f92672>=</span> (pkgs<span style=color:#f92672>.</span>buildFHSEnv {
</span></span><span style=display:flex><span>          name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;nvidia-fuck-you&#34;</span>;
</span></span><span style=display:flex><span>          targetPkgs <span style=color:#f92672>=</span> pkgs: (<span style=color:#66d9ef>with</span> pkgs; [
</span></span><span style=display:flex><span>            linuxPackages<span style=color:#f92672>.</span>nvidia_x11
</span></span><span style=display:flex><span>            libGLU libGL
</span></span><span style=display:flex><span>            xorg<span style=color:#f92672>.</span>libXi xorg<span style=color:#f92672>.</span>libXmu freeglut
</span></span><span style=display:flex><span>            xorg<span style=color:#f92672>.</span>libXext xorg<span style=color:#f92672>.</span>libX11 xorg<span style=color:#f92672>.</span>libXv xorg<span style=color:#f92672>.</span>libXrandr zlib 
</span></span><span style=display:flex><span>            ncurses5 stdenv<span style=color:#f92672>.</span>cc binutils
</span></span><span style=display:flex><span>            ffmpeg
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># I daily drive the fish shell</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># you can remove this, the default is bash</span>
</span></span><span style=display:flex><span>            fish
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Micromamba does the real legwork</span>
</span></span><span style=display:flex><span>            micromamba
</span></span><span style=display:flex><span>          ]);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          profile <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              export LD_LIBRARY_PATH=&#34;</span><span style=color:#e6db74>${</span>pkgs<span style=color:#f92672>.</span>linuxPackages<span style=color:#f92672>.</span>nvidia_x11<span style=color:#e6db74>}</span><span style=color:#e6db74>/lib&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              export CUDA_PATH=&#34;</span><span style=color:#e6db74>${</span>pkgs<span style=color:#f92672>.</span>cudatoolkit<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              export EXTRA_LDFLAGS=&#34;-L/lib -L</span><span style=color:#e6db74>${</span>pkgs<span style=color:#f92672>.</span>linuxPackages<span style=color:#f92672>.</span>nvidia_x11<span style=color:#e6db74>}</span><span style=color:#e6db74>/lib&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              export EXTRA_CCFLAGS=&#34;-I/usr/include&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          &#39;&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          <span style=color:#75715e># again, you can remove this if you like bash</span>
</span></span><span style=display:flex><span>          runScript <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;fish&#34;</span>;
</span></span><span style=display:flex><span>        })<span style=color:#f92672>.</span>env;
</span></span><span style=display:flex><span>      };
</span></span><span style=display:flex><span>    }
</span></span></code></pre></div><blockquote><p><em>Note:</em> This is <em>NOT</em> the same as containers. The most obvious way to tell is because
you can access your NVIDIA GPU as is, without any passthrough shenanigans.</p></blockquote><p>Enter this flake development environment using <code>nix develop</code>.</p><h1 id=setting-up-pytorch>Setting up PyTorch</h1><p>Now that we have the scaffolding, we can use <code>micromamba</code> to install CUDA for our ML tooling.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>micromamba env create <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -n my-environment <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    anaconda::cudatoolkit <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    anaconda::cudnn <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    <span style=color:#e6db74>&#34;anaconda::pytorch=*=*cuda*&#34;</span>
</span></span></code></pre></div><p>Here I&rsquo;m creating an environment called <code>my-environment</code> with <code>cudatoolkit</code>, <code>cudnn</code> and PyTorch. While installing PyTorch, make sure to
pick a version whose name contains &ldquo;cuda&rdquo; like I did here, otherwise, it defaults to the CPU version.</p><p>You can also define a <code>micromamba</code> environment with a config file. Read more about it <a href=https://conda.io/projects/conda/en/latest/user-guide/manage-environments.html>here</a>.</p><p>Once the env gets created, use <code>micromamba activate my-environment</code> to hop right in. Profit!</p><h1 id=conclusion>Conclusion</h1><p>Although this is not the Nix way of doing things, with micromamba being imeperative, this is probably the quickest
and most hassle free experience to start ML stuff on NixOS. I&rsquo;ve seen quite a lot of friends giving up on NixOS because of how annoying closed source libraries like CUDA can be.</p><p>Share this article around if you found this hacky approach to have improved your developer experience. I&rsquo;m banking on open source alternatives to pick up steam
so that hopefully this article becomes irrelevant in the future.</p><p>Bye now.</p></section></article><footer class=post-tags data-pagefind-meta=tags><a href=https://lavafroth.is-a.dev/tags/nix class=list-tag>Nix</a>
<a href=https://lavafroth.is-a.dev/tags/nixos class=list-tag>NixOS</a>
<a href=https://lavafroth.is-a.dev/tags/machine-learning class=list-tag>Machine Learning</a>
<a href=https://lavafroth.is-a.dev/tags/python class=list-tag>Python</a>
<a href=https://lavafroth.is-a.dev/tags/workflow class=list-tag>Workflow</a>
<a href=https://lavafroth.is-a.dev/tags/nvidia class=list-tag>NVIDIA</a>
<a href=https://lavafroth.is-a.dev/tags/cuda class=list-tag>CUDA</a>
<a href=https://lavafroth.is-a.dev/tags/rant class=list-tag>Rant</a></footer><nav class=post-nav><a class=prev href=https://lavafroth.is-a.dev/post/the-gsoc-grand-finale/><span>←</span><span>Wrapping up GSoC 2024</span></a>
<a class=next href=https://lavafroth.is-a.dev/post/how-i-use-swhkd-in-my-workflow/><span>How I Use SWHKD in My Workflow</span><span>→</span></a></nav></main><footer class=footer><p>&copy; 2025 <a href=https://lavafroth.is-a.dev/>lavafroth</a></p><p><a href=https://github.com/lavafroth/lavafroth.github.io/issues/new/choose>Report an issue</a></p><p><a href=https://github.com/lavafroth/lavafroth.github.io/discussions/>Discuss</a></p><p><a href=https://lavafroth.is-a.dev/privacy>Privacy</a></p><p><a href=https://creativecommons.org/licenses/by-sa/4.0/legalcode>License</a></p></footer></body></html>