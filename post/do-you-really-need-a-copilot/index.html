<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Need a hand? - lavafroth</title><meta name=description content='The tides
Over the past few months, a sizable fraction of my developer peers have taken to
AI tools. Beckoned from under a rock by the light of day, I was taken aback by this rising wave of vibe coding.
They claim AI tools to be phenomenal for frontend technologies like
React and NextJS. The selling point? Context aware autocompletes and agent mode.
Context aware autocompletes happen when the model watches your code so it can
suggest autocompletes while you code.'><meta name=author content><link rel="preload stylesheet" as=style href=https://lavafroth.is-a.dev/app.min.css><link rel="preload stylesheet" href=../../header.svg><noscript><link rel="preload stylesheet" as=style href=https://lavafroth.is-a.dev/noscript.min.css></noscript><link rel=preload as=font href=https://lavafroth.is-a.dev/LeagueGothic.ttf><link as=font href=https://lavafroth.is-a.dev/latinmodern-math.otf><link rel=preload as=image href=https://lavafroth.is-a.dev/github.svg><link rel=preload as=image href=https://lavafroth.is-a.dev/about.svg><link rel=preload as=image href=https://lavafroth.is-a.dev/art.svg><link rel=icon href=https://lavafroth.is-a.dev/favicon.png><link rel=blog-icon href=https://lavafroth.is-a.dev/icon.png></head><body class=not-ready data-menu=true><header class=header><h1><a class=site-name href=https://lavafroth.is-a.dev/>lavafroth</a></h1><svg id="btnDark" class="btn-dark" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 60 60" preserveAspectRatio="xMinYMin meet"><path class="circle" d="M30 16c7.73.0 14 6.27 14 14 0 3.81-1.53 7.27-4 9.8-2.54 2.59-6.08 4.2-10 4.2-7.73.0-14-6.27-14-14 0-7.73 6.27-14 14-14z"/><g class="rays" data-svg-origin="30 30"><line class="line" x1="30" y1="11" x2="30" y2="7"/><line class="line" x1="43.44" y1="16.57" x2="46.26" y2="13.74"/><line class="line" x1="49" y1="30" x2="53" y2="30"/><line class="line" x1="43.43" y1="43.44" x2="46.26" y2="46.26"/><line class="line" x1="30" y1="49" x2="30" y2="53"/><line class="line" x1="16.56" y1="43.43" x2="13.74" y2="46.26"/><line class="line" x1="11" y1="30" x2="7" y2="30"/><line class="line" x1="16.57" y1="16.56" x2="13.74" y2="13.74"/></g></svg><div class=space></div><a style=--url:url(./github.svg) href=https://github.com/lavafroth aria-label=github target=_blank></a><a href=../../about/ aria-label=about style=--url:url(./about.svg)></a><a href=../../art/ aria-label=art style=--url:url(./art.svg)></a><script>const bodyClx=document.body.classList,sysDark=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark"),setDark=e=>(bodyClx.toggle("dark",e),localStorage.setItem("dark",e));setDark(darkVal?darkVal==="true":sysDark.matches),requestAnimationFrame(()=>bodyClx.remove("not-ready")),btnDark.addEventListener("click",()=>setDark(!bodyClx.contains("dark"))),sysDark.addEventListener("change",({matches:e})=>setDark(e))</script></header><main class=main><article class=post-single><header class=post-title><p><time>Apr 3, 2025 | 4 minutes read</time></p><h1 data-pagefind-meta=title>Need a hand?</h1></header><section class=post-content data-pagefind-body><h2 id=the-tides>The tides</h2><p>Over the past few months, a sizable fraction of my developer peers have taken to
AI tools. Beckoned from under a rock by the light of day, I was taken aback by this rising wave of <em>vibe coding</em>.</p><p>They claim AI tools to be phenomenal for frontend technologies like
React and NextJS. The selling point? Context aware autocompletes and agent mode.</p><p>Context aware autocompletes happen when the model watches your code so it can
suggest autocompletes while you code.</p><p>Agent mode involves an <em>&ldquo;autonomous agent&rdquo;</em> based on a prompt that can
manipulate files, run commands and essentially carve out a project.</p><h2 id=impressionable>Impressionable</h2><p>When I ask my friends about how LLMs improve their code, it is
often simple, easy to spot errors that would have been obvious by reading the documentation.
Thus, the LLMs really provide us an artificial sense of competence.</p><p>Here&rsquo;s what my friend @noobscience had to say:</p><blockquote><p>The place where it helps me the most is simple errors.
Like, let&rsquo;s say I forgot to await a promise, it pretty much finds that out.</p></blockquote><p>A good first question is &ldquo;what is the scope of a Copilot?&rdquo;</p><p>Is the purpose simply to write boilerplate code and small fixes?</p><p>Most reasonable developers would not want to surrender the steering wheel
completely to the LLM because of their unpredictability. Good luck
trying to fix bugs with little to no clue about how your own code works.</p><h2 id=a-cheap-knock-off>A cheap knock-off</h2><p>A deeper question concerns the tooling used during development.</p><p>When most people admit that they use LLMs to catch small mistakes and fix them
predictably, what they actually mean is that these models serve as cheap, less
predictable knock-offs of more robust, deterministic and less resource hungry
language tooling.</p><p>Developers needing LLMs assistance is a symptom of the compiler or language
tooling being subpar. That they are not good enough to catch low hanging errors.</p><p>In fact, quite a few languages provide fantastic tooling and make programming
feel incredibly pleasant. Some good examples include <code>gopls</code> for the Go
programming language and <code>rust-analyzer</code> for Rust.</p><p>To conclude, I think what developers really want are crude, simple yet battle
tested tooling instead of big blobs that take three nukes&rsquo; worth of energy to
train and 4 GPUs to run.</p><hr><p>Update: 2025-06-15</p><h2 id=thicc-macros>Thicc macros</h2><p>After pondering on the status quo for a couple days, I have come to consider
AI code as thick, probabilistic macros. Their drawbacks surface when they eventually
output poor quality code.</p><p>I do like the idea of having comments that describe the steps to a problem and having
code generated for it, by a human or otherwise. I tried to distill the idea of such code
retrieval and after putting a lot of thought into its architecture, I&rsquo;m happy to announce Silos.</p><h2 id=silos>Silos</h2><p>Silos is a simple server that runs a small embedding model to embed queries, find the top <span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span> matches and respond with the output in JSON. You can give it a try <a href=https://github.com/lavafroth/silos>here</a>.</p><p>Here&rsquo;s how it differs from LLMs</p><ul><li>Snippets are the building blocks: Silos ingests and stores snippets, each with an associated description and language.</li><li>Absence of context: You are on your own, snippet queried must be small and as self-contained as possible. While it allows placeholders for variables, it does not hyperspecialize the code for your codebase.</li><li>Runs on ancient hardware: Silos v1 uses all-MiniLM for embedding queries, it&rsquo;s a fairly small model with a memory footprint of 50MiB.</li><li>Local first: One can easily setup Silos on local machines for offline work. I do plan to host a central repository and API for community snippets.</li><li>Code, your way: Don&rsquo;t like the code style of the existing snippets? Feel free to add your own! You can add custom snippets using the REST API ephemerally or add them to the snippets directory for persistent use.</li></ul><h3 id=fun-fact>Fun fact</h3><p>Before public development, the project was named SnippetHub. Considering all the other <abbr title="By that I mean DockerHub, FlakeHub, etc. Get your mind out of the gutter."><em>hubs</em></abbr> around, I switched it up to be Silos because the snippets are in self-contained silos.</p></section></article><footer class=post-tags data-pagefind-meta=tags><a href=https://lavafroth.is-a.dev/tags/llm class=list-tag>LLM</a>
<a href=https://lavafroth.is-a.dev/tags/ai class=list-tag>AI</a>
<a href=https://lavafroth.is-a.dev/tags/rant class=list-tag>Rant</a>
<a href=https://lavafroth.is-a.dev/tags/copilot class=list-tag>Copilot</a></footer><nav class=post-nav><a class=prev href=https://lavafroth.is-a.dev/post/detecting-stripped-go-binaries/><span>←</span><span>Easy grep to detect stripped Go binaries</span></a>
<a class=next href=https://lavafroth.is-a.dev/post/in-search-of-the-smallest-dna-compl/><span>In search of the smallest DNA complement function</span><span>→</span></a></nav></main><footer class=footer><p>&copy; 2025 <a href=https://lavafroth.is-a.dev/>lavafroth</a></p><p><a href=https://github.com/lavafroth/lavafroth.github.io/issues/new/choose>Report an issue</a></p><p><a href=https://github.com/lavafroth/lavafroth.github.io/discussions/>Discuss</a></p><p><a href=https://lavafroth.is-a.dev/privacy>Privacy</a></p><p><a href=https://creativecommons.org/licenses/by-sa/4.0/legalcode>License</a></p></footer></body></html>